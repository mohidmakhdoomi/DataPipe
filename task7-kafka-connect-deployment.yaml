apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-connect-config
  namespace: data-ingestion
  labels:
    app: kafka-connect
    component: configuration
data:
  # Kafka Connect Worker Configuration
  connect-distributed.properties: |
    # Kafka Connection
    bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    
    # Worker Configuration
    group.id=connect-cluster
    
    # Internal Topics (created by topic setup job)
    config.storage.topic=connect-configs
    config.storage.replication.factor=3
    offset.storage.topic=connect-offsets
    offset.storage.replication.factor=3
    offset.storage.partitions=25
    status.storage.topic=connect-status
    status.storage.replication.factor=3
    status.storage.partitions=5
    
    # Converters - Avro with Schema Registry
    key.converter=io.confluent.connect.avro.AvroConverter
    value.converter=io.confluent.connect.avro.AvroConverter
    key.converter.schema.registry.url=http://schema-registry.data-ingestion.svc.cluster.local:8081
    value.converter.schema.registry.url=http://schema-registry.data-ingestion.svc.cluster.local:8081
    
    # Internal Converters
    internal.key.converter=org.apache.kafka.connect.json.JsonConverter
    internal.value.converter=org.apache.kafka.connect.json.JsonConverter
    internal.key.converter.schemas.enable=false
    internal.value.converter.schemas.enable=false
    
    # REST API Configuration
    rest.port=8083
    rest.host.name=0.0.0.0
    rest.advertised.host.name=kafka-connect
    rest.advertised.port=8083
    
    # Plugin Configuration
    plugin.path=/kafka/connect,/usr/share/java/kafka,/usr/share/java/confluent-common
    
    # Worker Settings
    task.shutdown.graceful.timeout.ms=10000
    offset.flush.interval.ms=10000
    offset.flush.timeout.ms=5000
    
    # Producer Settings
    producer.bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    producer.compression.type=lz4
    producer.max.request.size=1048576
    producer.buffer.memory=33554432
    producer.batch.size=16384
    producer.linger.ms=10
    producer.acks=1
    
    # Consumer Settings
    consumer.bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    consumer.max.poll.records=10000
    consumer.max.poll.interval.ms=180000
    consumer.session.timeout.ms=30000
    consumer.heartbeat.interval.ms=3000
    consumer.auto.offset.reset=earliest
    
    # Error Handling
    errors.retry.timeout=300000
    errors.retry.delay.max.ms=60000
    errors.tolerance=none
    
    # Config Providers for ${file:...} variable resolution
    config.providers=file
    config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider
    
    # Logging
    connect.logs.dir=/kafka/logs
    
  # Log4j Configuration
  connect-log4j.properties: |
    log4j.rootLogger=INFO, stdout, connectAppender
    
    # Console appender
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n
    
    # Connect appender
    log4j.appender.connectAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.connectAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.connectAppender.File=/kafka/logs/connect.log
    log4j.appender.connectAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.connectAppender.layout.ConversionPattern=[%d] %p %m (%c)%n
    
    # Reduce verbosity for some packages
    log4j.logger.org.apache.kafka.connect.runtime.WorkerSourceTask=WARN
    log4j.logger.org.apache.kafka.connect.runtime.WorkerSinkTask=WARN
    log4j.logger.org.reflections=ERROR

  # JVM Configuration based on multi-model consensus
  KAFKA_HEAP_OPTS: "-Xms128m -Xmx512m"
  JAVA_TOOL_OPTIONS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+ExitOnOutOfMemoryError"
  CONNECT_BOOTSTRAP_SERVERS: "kafka-headless.data-ingestion.svc.cluster.local:9092"
  CONNECT_GROUP_ID: "connect-cluster"
  CONNECT_CONFIG_STORAGE_TOPIC: "connect-configs"
  CONNECT_OFFSET_STORAGE_TOPIC: "connect-offsets"
  CONNECT_STATUS_STORAGE_TOPIC: "connect-status"
  CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
  CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
  CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry.data-ingestion.svc.cluster.local:8081"
  CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry.data-ingestion.svc.cluster.local:8081"
  CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
  CONNECT_REST_PORT: "8083"
  CONNECT_PLUGIN_PATH: "/kafka/connect,/usr/share/java/kafka,/usr/share/java/confluent-common"
  CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
  KAFKA_OPTS: "-Dlog4j.configuration=file:/kafka/config/connect-log4j.properties"
  KAFKA_BOOTSTRAP_SERVERS: "kafka-headless.data-ingestion.svc.cluster.local:9092"
  CONFLUENT_TELEMETRY_ENABLED: "false"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-connect
  namespace: data-ingestion
  labels:
    app: kafka-connect
    component: worker
spec:
  replicas: 1  # Single worker based on multi-model consensus
  selector:
    matchLabels:
      app: kafka-connect
      component: worker
  template:
    metadata:
      labels:
        app: kafka-connect
        component: worker
    spec:
      serviceAccountName: kafka-connect-sa
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      initContainers:
      # Install Debezium and S3 Sink plugins
      - name: plugin-installer
        image: curlimages/curl
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        command:
        - sh
        - -c
        - |
          set -e
          echo "Installing connector plugins..."
          
          # Create plugin directories
          mkdir -p /kafka/connect/debezium-postgres
          mkdir -p /kafka/connect/s3-sink
          cd /tmp
          
          # Download and install Debezium PostgreSQL connector
          echo "Installing Debezium PostgreSQL connector plugin..."
          curl -L -o debezium-connector-postgresql.zip \
            "https://api.hub.confluent.io/api/plugins/debezium/debezium-connector-postgresql/versions/2.4.2/archive"
          
          unzip -q debezium-connector-postgresql.zip
          cp -r debezium-debezium-connector-postgresql-2.4.2/* /kafka/connect/debezium-postgres/
          
          echo "Debezium PostgreSQL connector installed successfully!"
          
          # Download and install S3 Sink connector
          echo "Installing S3 Sink connector plugin..."
          curl -L -o kafka-connect-s3.zip \
            "https://api.hub.confluent.io/api/plugins/confluentinc/kafka-connect-s3/versions/10.5.0/archive"
          
          unzip -q kafka-connect-s3.zip
          cp -r confluentinc-kafka-connect-s3-10.5.0/* /kafka/connect/s3-sink/
          
          echo "S3 Sink connector installed successfully!"
          echo "Installed plugins:"
          ls -la /kafka/connect/
        volumeMounts:
        - name: plugin-volume
          mountPath: /kafka/connect
      containers:
      - name: kafka-connect
        image: confluentinc/cp-kafka-connect:7.4.0
        command: ["/bin/bash"]
        args: ["/etc/confluent/docker/run"]
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        envFrom:
        - configMapRef:
            name: kafka-connect-config
        ports:
        - name: http
          containerPort: 8083
        resources:
          requests:
            memory: "256Mi"
            cpu: "500m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /
            port: 8083
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /connectors
            port: 8083
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /
            port: 8083
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30  # Total: 30 + (30*10) = 330s (5.5 minutes)
        volumeMounts:
        - name: config-volume
          mountPath: /kafka/config
        - name: plugin-volume
          mountPath: /kafka/connect
        - name: logs-volume
          mountPath: /kafka/logs
        - name: aws-credentials
          mountPath: /etc/aws-credentials
          readOnly: true
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: access-key-id
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: secret-access-key
        - name: AWS_DEFAULT_REGION
          valueFrom:
            secretKeyRef:
              name: aws-credentials
              key: region
      volumes:
      - name: config-volume
        configMap:
          name: kafka-connect-config
      - name: plugin-volume
        emptyDir: {}
      - name: logs-volume
        emptyDir: {}
      - name: aws-credentials
        secret:
          secretName: aws-credentials
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-connect
  namespace: data-ingestion
  labels:
    app: kafka-connect
    component: service
spec:
  type: ClusterIP
  selector:
    app: kafka-connect
    component: worker
  ports:
  - name: http
    protocol: TCP
    port: 8083
    targetPort: 8083
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-connect-nodeport
  namespace: data-ingestion
  labels:
    app: kafka-connect
    component: external-service
spec:
  type: NodePort
  selector:
    app: kafka-connect
    component: worker
  ports:
  - name: http
    port: 8083
    targetPort: 8083
    nodePort: 30083
    protocol: TCP
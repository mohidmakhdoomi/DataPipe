apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-connect-config
  namespace: data-ingestion
  labels:
    app: kafka-connect
    component: configuration
data:
  # Kafka Connect Worker Configuration
  connect-distributed.properties: |
    # Kafka Connection
    bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    
    # Worker Configuration
    group.id=connect-cluster
    
    # Internal Topics (created by topic setup job)
    config.storage.topic=connect-configs
    config.storage.replication.factor=3
    offset.storage.topic=connect-offsets
    offset.storage.replication.factor=3
    offset.storage.partitions=25
    status.storage.topic=connect-status
    status.storage.replication.factor=3
    status.storage.partitions=5
    
    # Converters - Avro with Schema Registry
    key.converter=io.confluent.connect.avro.AvroConverter
    value.converter=io.confluent.connect.avro.AvroConverter
    key.converter.schema.registry.url=http://schema-registry.data-ingestion.svc.cluster.local:8081
    value.converter.schema.registry.url=http://schema-registry.data-ingestion.svc.cluster.local:8081
    
    # Internal Converters
    internal.key.converter=org.apache.kafka.connect.json.JsonConverter
    internal.value.converter=org.apache.kafka.connect.json.JsonConverter
    internal.key.converter.schemas.enable=false
    internal.value.converter.schemas.enable=false
    
    # REST API Configuration
    rest.port=8083
    rest.host.name=0.0.0.0
    rest.advertised.host.name=kafka-connect
    rest.advertised.port=8083
    
    # Plugin Configuration
    plugin.path=/kafka/connect,/usr/share/java,/usr/share/confluent-hub-components
    
    # Worker Settings
    task.shutdown.graceful.timeout.ms=10000
    offset.flush.interval.ms=10000
    offset.flush.timeout.ms=5000
    
    # Producer Settings
    producer.bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    producer.compression.type=lz4
    producer.max.request.size=1048576
    producer.buffer.memory=33554432
    producer.batch.size=16384
    producer.linger.ms=10
    producer.acks=1
    
    # Consumer Settings
    consumer.bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    consumer.max.poll.records=500
    consumer.max.poll.interval.ms=300000
    consumer.session.timeout.ms=30000
    consumer.heartbeat.interval.ms=3000
    consumer.auto.offset.reset=earliest
    
    # Error Handling
    errors.retry.timeout=300000
    errors.retry.delay.max.ms=60000
    errors.tolerance=none
    
    # Logging
    connect.logs.dir=/kafka/logs
    
  # Log4j Configuration
  connect-log4j.properties: |
    log4j.rootLogger=INFO, stdout, connectAppender
    
    # Console appender
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n
    
    # Connect appender
    log4j.appender.connectAppender=org.apache.log4j.DailyRollingFileAppender
    log4j.appender.connectAppender.DatePattern='.'yyyy-MM-dd-HH
    log4j.appender.connectAppender.File=/kafka/logs/connect.log
    log4j.appender.connectAppender.layout=org.apache.log4j.PatternLayout
    log4j.appender.connectAppender.layout.ConversionPattern=[%d] %p %m (%c)%n
    
    # Reduce verbosity for some packages
    log4j.logger.org.apache.kafka.connect.runtime.WorkerSourceTask=WARN
    log4j.logger.org.apache.kafka.connect.runtime.WorkerSinkTask=WARN
    log4j.logger.org.reflections=ERROR
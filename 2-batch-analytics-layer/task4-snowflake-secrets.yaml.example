# Snowflake Connection Credentials and Configuration for Batch Analytics Layer
# These secrets provide Snowflake access for data warehouse operations
# Replace placeholder values with actual Snowflake credentials

apiVersion: v1
kind: Secret
metadata:
  name: snowflake-credentials
  namespace: batch-analytics
  labels:
    app: snowflake
    component: credentials
type: Opaque
stringData:
  # Snowflake Connection Parameters - Replace with actual values
  SNOWFLAKE_ACCOUNT: "your-account.snowflakecomputing.com"
  SNOWFLAKE_USER: "BATCH_ANALYTICS_USER"
  SNOWFLAKE_PASSWORD: "your_secure_password_here"
  SNOWFLAKE_ROLE: "TRANSFORMER"
  SNOWFLAKE_WAREHOUSE: "COMPUTE_WH"
  SNOWFLAKE_DATABASE: "ECOMMERCE_DW"
  SNOWFLAKE_SCHEMA: "RAW"
  
  # Connection Options
  SNOWFLAKE_TIMEZONE: "UTC"
  SNOWFLAKE_CLIENT_SESSION_KEEP_ALIVE: "true"
  SNOWFLAKE_CLIENT_PREFETCH_THREADS: "4"
  
  # Spark Snowflake Connector Configuration
  SNOWFLAKE_URL: "your-account.snowflakecomputing.com"
  SNOWFLAKE_WAREHOUSE_SIZE: "SMALL"
  SNOWFLAKE_AUTO_SUSPEND: "300"  # 5 minutes
  SNOWFLAKE_AUTO_RESUME: "true"
---
# ConfigMap for Snowflake connection configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: snowflake-config
  namespace: batch-analytics
  labels:
    app: snowflake
    component: configuration
data:
  # Snowflake Connection Properties
  snowflake-connection.properties: |
    # Connection Settings
    sfURL=your-account.snowflakecomputing.com
    sfWarehouse=COMPUTE_WH
    sfDatabase=ECOMMERCE_DW
    sfSchema=RAW
    sfRole=TRANSFORMER
    
    # Performance Settings
    sfCompress=on
    sfSSL=on
    sfTimezone=UTC
    
    # Connection Pool Settings
    sfMaxPoolSize=10
    sfMinPoolSize=1
    sfMaxIdleTime=300
    sfConnectionTimeout=60
    sfNetworkTimeout=300
    
    # Query Settings
    sfQueryTimeout=3600
    sfMaxRetries=3
    sfRetryTimeout=300
    
  # Spark Snowflake Connector Configuration
  spark-snowflake.conf: |
    # Snowflake Spark Connector Settings
    # spark.jars.packages=net.snowflake:spark-snowflake_2.12:3.1.5
    spark.sql.adaptive.enabled=true
    spark.sql.adaptive.coalescePartitions.enabled=true
    
    # Snowflake-specific Spark configurations
    spark.snowflake.keep_column_case=off
    spark.snowflake.autopushdown=on
    spark.snowflake.use_copy_unload=true
    spark.snowflake.streaming_stage=BATCH_ANALYTICS_STAGE
    
    # Performance tuning for Snowflake operations
    spark.executor.memory=4g
    spark.executor.cores=2
    spark.sql.shuffle.partitions=200
    spark.serializer=org.apache.spark.serializer.KryoSerializer
    
  # 3-Layer Architecture Schema Definitions
  schema-definitions.sql: |
    -- Database and Schema Creation
    CREATE DATABASE IF NOT EXISTS ECOMMERCE_DW;
    USE DATABASE ECOMMERCE_DW;
    
    -- Raw Schema for direct data ingestion
    CREATE SCHEMA IF NOT EXISTS RAW;
    
    -- Staging Schema for data quality and business rules
    CREATE SCHEMA IF NOT EXISTS STAGING;
    
    -- Marts Schema for business-ready data models
    CREATE SCHEMA IF NOT EXISTS MARTS;
    
    -- Create warehouse with auto-suspend
    CREATE WAREHOUSE IF NOT EXISTS COMPUTE_WH
    WITH WAREHOUSE_SIZE = 'SMALL'
    AUTO_SUSPEND = 300
    AUTO_RESUME = TRUE
    INITIALLY_SUSPENDED = TRUE;
    
    -- Create role and user for batch analytics
    CREATE ROLE IF NOT EXISTS TRANSFORMER;
    CREATE USER IF NOT EXISTS BATCH_ANALYTICS_USER
    PASSWORD = 'CHANGE_ME'
    DEFAULT_ROLE = 'TRANSFORMER'
    DEFAULT_WAREHOUSE = 'COMPUTE_WH'
    DEFAULT_NAMESPACE = 'ECOMMERCE_DW.RAW';
    
    -- Grant permissions
    GRANT USAGE ON WAREHOUSE COMPUTE_WH TO ROLE TRANSFORMER;
    GRANT USAGE ON DATABASE ECOMMERCE_DW TO ROLE TRANSFORMER;
    GRANT ALL ON SCHEMA ECOMMERCE_DW.RAW TO ROLE TRANSFORMER;
    GRANT ALL ON SCHEMA ECOMMERCE_DW.STAGING TO ROLE TRANSFORMER;
    GRANT ALL ON SCHEMA ECOMMERCE_DW.MARTS TO ROLE TRANSFORMER;
    GRANT ROLE TRANSFORMER TO USER BATCH_ANALYTICS_USER;
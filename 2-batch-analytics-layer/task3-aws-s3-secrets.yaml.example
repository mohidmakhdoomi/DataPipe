# AWS S3 Credentials and Configuration for Batch Analytics Layer
# These secrets provide S3 access for Iceberg data lake operations
# Replace placeholder values with actual AWS credentials

apiVersion: v1
kind: Secret
metadata:
  name: aws-s3-credentials
  namespace: batch-analytics
  labels:
    app: aws-s3
    component: credentials
type: Opaque
stringData:
  # AWS Access Credentials - Replace with actual values
  AWS_ACCESS_KEY_ID: "AKIA_PLACEHOLDER_ACCESS_KEY"
  AWS_SECRET_ACCESS_KEY: "placeholder_secret_access_key_value"
  AWS_DEFAULT_REGION: "us-east-1"
  
  # S3 Configuration
  AWS_S3_BUCKET: "data-s3-bucket"
  AWS_S3_ENDPOINT: "https://s3.amazonaws.com"
  
  # Iceberg Warehouse Configuration
  ICEBERG_WAREHOUSE_PATH: "s3://data-s3-bucket/iceberg-warehouse/"
  ICEBERG_CATALOG_TYPE: "hadoop"
---
# ConfigMap for S3 and Iceberg configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: s3-iceberg-config
  namespace: batch-analytics
  labels:
    app: s3-iceberg
    component: configuration
data:
  # S3 Configuration Properties
  s3-config.properties: |
    # S3 Connection Settings
    fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    fs.s3a.endpoint=s3.amazonaws.com
    fs.s3a.path.style.access=false
    
    # Performance Optimization
    fs.s3a.connection.maximum=200
    fs.s3a.threads.max=64
    fs.s3a.connection.establish.timeout=5000
    fs.s3a.connection.timeout=200000
    fs.s3a.attempts.maximum=10
    fs.s3a.retry.interval=500ms
    
    # Multipart Upload Configuration
    fs.s3a.multipart.size=104857600
    fs.s3a.multipart.threshold=134217728
    fs.s3a.fast.upload=true
    fs.s3a.fast.upload.buffer=disk
    fs.s3a.fast.upload.active.blocks=8
    
    # Server-side Encryption
    fs.s3a.server-side-encryption-algorithm=AES256
    fs.s3a.server-side-encryption.key=
    
  # Iceberg Catalog Configuration
  iceberg-catalog.properties: |
    # Iceberg Hadoop Catalog Configuration
    catalog-impl=org.apache.iceberg.hadoop.HadoopCatalog
    warehouse=s3://data-s3-bucket/iceberg-warehouse/
    
    # File Format Configuration
    write.format.default=parquet
    write.parquet.compression-codec=snappy
    write.metadata.compression-codec=gzip
    write.target-file-size-bytes=134217728
    
    # Table Properties
    commit.retry.num-retries=4
    commit.retry.min-wait-ms=100
    commit.retry.max-wait-ms=60000
    
    # Snapshot Management
    history.expire.max-snapshot-age-ms=432000000
    history.expire.min-snapshots-to-keep=100

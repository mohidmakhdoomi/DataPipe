apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: snowflake-connectivity-test
  namespace: batch-analytics
  labels:
    app: snowflake-test
    component: connectivity-test
spec:
  type: Scala
  mode: cluster
  image: "spark:3.5.7-hadoop-aws-iceberg-snowflake"
  imagePullPolicy: Never
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: "local:///opt/spark/examples/jars/spark-examples_2.12-3.4.1.jar"
  sparkVersion: 3.5.7
  restartPolicy:
    type: Never
  
  # Resource allocation for connectivity test
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "2g"
    memoryOverhead: "512m"
    serviceAccount: spark-driver-sa
    podSecurityContext:
      fsGroup: 185
      fsGroupChangePolicy: "OnRootMismatch"
    env:
    - name: SNOWFLAKE_ACCOUNT
      valueFrom:
        secretKeyRef:
          name: snowflake-credentials
          key: SNOWFLAKE_ACCOUNT
    - name: SNOWFLAKE_USER
      valueFrom:
        secretKeyRef:
          name: snowflake-credentials
          key: SNOWFLAKE_USER
    - name: SNOWFLAKE_PASSWORD
      valueFrom:
        secretKeyRef:
          name: snowflake-credentials
          key: SNOWFLAKE_PASSWORD
    - name: SNOWFLAKE_ROLE
      valueFrom:
        secretKeyRef:
          name: snowflake-credentials
          key: SNOWFLAKE_ROLE
    - name: SNOWFLAKE_WAREHOUSE
      valueFrom:
        secretKeyRef:
          name: snowflake-credentials
          key: SNOWFLAKE_WAREHOUSE
    - name: SNOWFLAKE_DATABASE
      valueFrom:
        secretKeyRef:
          name: snowflake-credentials
          key: SNOWFLAKE_DATABASE
    - name: SNOWFLAKE_SCHEMA
      valueFrom:
        secretKeyRef:
          name: snowflake-credentials
          key: SNOWFLAKE_SCHEMA
    volumeMounts:
    - name: event-log-volume
      mountPath: /var/spark-events
    - name: snowflake-config
      mountPath: /opt/spark/conf/snowflake
      readOnly: true
  
  executor:
    cores: 1
    instances: 1
    memory: "2g"
    memoryOverhead: "512m"
    serviceAccount: spark-executor-sa
    podSecurityContext:
      fsGroup: 185
      fsGroupChangePolicy: "OnRootMismatch"
    deleteOnTermination: false
    volumeMounts:
    - name: event-log-volume
      mountPath: /var/spark-events
  
  # Spark configuration for Snowflake connectivity
  sparkConf:
    "spark.eventLog.enabled": "true"
    "spark.eventLog.dir": "file:///var/spark-events"

    # "spark.jars.packages": "net.snowflake:spark-snowflake_2.12:3.1.5"
    "spark.sql.adaptive.enabled": "true"
    "spark.sql.adaptive.coalescePartitions.enabled": "true"
    "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
    "spark.sql.extensions": "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions"
    "spark.snowflake.keep_column_case": "off"
    "spark.snowflake.autopushdown": "on"
    "spark.executor.extraJavaOptions": "-Dcom.amazonaws.services.s3.enableV4=true"
    "spark.driver.extraJavaOptions": "-Dcom.amazonaws.services.s3.enableV4=true"
  
  # Hadoop configuration for S3 access
  hadoopConf:
    "fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "fs.s3a.aws.credentials.provider": "com.amazonaws.auth.EnvironmentVariableCredentialsProvider"
    "fs.s3a.endpoint": "s3.amazonaws.com"
    "fs.s3a.path.style.access": "false"
    "fs.s3a.connection.maximum": "200"
    "fs.s3a.multipart.size": "104857600"
    "fs.s3a.fast.upload": "true"
  
  volumes:
  - name: event-log-volume
    persistentVolumeClaim:
      claimName: spark-history-pvc
  - name: snowflake-config
    configMap:
      name: snowflake-config
  
  # # Monitoring and debugging
  # monitoring:
  #   exposeDriverMetrics: true
  #   exposeExecutorMetrics: true
  #   prometheus:
  #     jmxExporterJar: "/prometheus/jmx_prometheus_javaagent-0.17.2.jar"
  #     port: 8090
  
  # # Node selector for batch processing
  # nodeSelector:
  #   kubernetes.io/os: linux
apiVersion: batch/v1
kind: Job
metadata:
  name: spark-etl-job
  namespace: data-pipeline
  labels:
    app: spark-etl
spec:
  template:
    metadata:
      labels:
        app: spark-etl
    spec:
      serviceAccountName: spark-service-account
      restartPolicy: Never
      containers:
      - name: spark-driver
        image: data-pipeline/spark-jobs:latest
        env:
        - name: SPARK_MODE
          value: "driver"
        - name: SPARK_APPLICATION_PYTHON_LOCATION
          value: "/app/jobs/etl_job.py"
        - name: SPARK_SUBMIT_OPTIONS
          value: >-
            --master k8s://https://kubernetes.default.svc:443
            --deploy-mode client
            --name spark-etl-job
            --conf spark.executor.instances=3
            --conf spark.executor.memory=2g
            --conf spark.executor.cores=2
            --conf spark.driver.memory=1g
            --conf spark.kubernetes.namespace=data-pipeline
            --conf spark.kubernetes.executor.container.image=data-pipeline/spark-jobs:latest
            --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark-service-account
            --conf spark.sql.adaptive.enabled=true
            --conf spark.sql.adaptive.coalescePartitions.enabled=true
            --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
            --conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain
        envFrom:
        - configMapRef:
            name: spark-config
        - secretRef:
            name: aws-credentials
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: spark-jobs
          mountPath: /app/jobs
      volumes:
      - name: spark-jobs
        configMap:
          name: spark-jobs-config

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-jobs-config
  namespace: data-pipeline
  labels:
    app: spark-jobs
data:
  etl_job.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    import os
    
    def create_spark_session():
        """Create Spark session with S3 configuration"""
        return SparkSession.builder \
            .appName("DataPipelineETL") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
            .getOrCreate()
    
    def process_kafka_data(spark):
        """Process data from Kafka (via S3) and prepare for Snowflake"""
        
        # Read raw data from S3 (Kafka Connect output)
        raw_bucket = os.getenv("S3_BUCKET_RAW", "data-lake-raw")
        processed_bucket = os.getenv("S3_BUCKET_PROCESSED", "data-lake-processed")
        
        print(f"Reading data from s3a://{raw_bucket}/kafka-data/")
        
        # Read transactions
        transactions_df = spark.read \
            .option("multiline", "true") \
            .json(f"s3a://{raw_bucket}/kafka-data/transactions/")
        
        # Read user events
        events_df = spark.read \
            .option("multiline", "true") \
            .json(f"s3a://{raw_bucket}/kafka-data/user_events/")
        
        # Process transactions
        processed_transactions = process_transactions(transactions_df)
        
        # Process user events
        processed_events = process_user_events(events_df)
        
        # Create aggregated metrics
        daily_metrics = create_daily_metrics(processed_transactions, processed_events)
        
        # Write processed data back to S3
        write_to_s3(processed_transactions, f"s3a://{processed_bucket}/transactions/")
        write_to_s3(processed_events, f"s3a://{processed_bucket}/user_events/")
        write_to_s3(daily_metrics, f"s3a://{processed_bucket}/daily_metrics/")
        
        print("ETL job completed successfully!")
    
    def process_transactions(df):
        """Clean and enrich transaction data"""
        return df.filter(col("status").isNotNull()) \
                 .withColumn("transaction_date", to_date(col("created_at"))) \
                 .withColumn("revenue_tier", 
                    when(col("total_amount") < 50, "Low")
                    .when(col("total_amount") < 200, "Medium")
                    .otherwise("High")) \
                 .withColumn("profit_margin", 
                    (col("total_amount") - col("discount_amount")) * 0.3) \
                 .dropDuplicates(["transaction_id"])
    
    def process_user_events(df):
        """Clean and enrich user event data"""
        return df.filter(col("event_type").isNotNull()) \
                 .withColumn("event_date", to_date(col("timestamp"))) \
                 .withColumn("hour_of_day", hour(col("timestamp"))) \
                 .withColumn("is_mobile", col("device_type") == "mobile") \
                 .dropDuplicates(["event_id"])
    
    def create_daily_metrics(transactions_df, events_df):
        """Create daily aggregated metrics"""
        
        # Daily transaction metrics
        daily_transactions = transactions_df.groupBy("transaction_date", "user_id") \
            .agg(
                sum("total_amount").alias("daily_revenue"),
                count("*").alias("transaction_count"),
                avg("total_amount").alias("avg_transaction_value")
            )
        
        # Daily event metrics
        daily_events = events_df.groupBy("event_date", "user_id") \
            .agg(
                count("*").alias("event_count"),
                countDistinct("session_id").alias("session_count"),
                sum(when(col("event_type") == "purchase", 1).otherwise(0)).alias("purchase_events")
            )
        
        # Join and create comprehensive daily metrics
        return daily_transactions.join(
            daily_events,
            (daily_transactions.transaction_date == daily_events.event_date) &
            (daily_transactions.user_id == daily_events.user_id),
            "full_outer"
        ).select(
            coalesce(daily_transactions.transaction_date, daily_events.event_date).alias("date"),
            coalesce(daily_transactions.user_id, daily_events.user_id).alias("user_id"),
            coalesce(col("daily_revenue"), lit(0)).alias("daily_revenue"),
            coalesce(col("transaction_count"), lit(0)).alias("transaction_count"),
            coalesce(col("avg_transaction_value"), lit(0)).alias("avg_transaction_value"),
            coalesce(col("event_count"), lit(0)).alias("event_count"),
            coalesce(col("session_count"), lit(0)).alias("session_count"),
            coalesce(col("purchase_events"), lit(0)).alias("purchase_events")
        )
    
    def write_to_s3(df, path):
        """Write DataFrame to S3 in Parquet format"""
        print(f"Writing data to {path}")
        df.coalesce(1) \
          .write \
          .mode("overwrite") \
          .option("compression", "snappy") \
          .parquet(path)
    
    if __name__ == "__main__":
        spark = create_spark_session()
        try:
            process_kafka_data(spark)
        finally:
            spark.stop()

  data_quality_job.py: |
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    import os
    
    def create_spark_session():
        """Create Spark session for data quality checks"""
        return SparkSession.builder \
            .appName("DataQualityChecks") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
    
    def run_data_quality_checks(spark):
        """Run comprehensive data quality checks"""
        
        processed_bucket = os.getenv("S3_BUCKET_PROCESSED", "data-lake-processed")
        
        # Read processed data
        transactions_df = spark.read.parquet(f"s3a://{processed_bucket}/transactions/")
        events_df = spark.read.parquet(f"s3a://{processed_bucket}/user_events/")
        
        # Data quality checks
        quality_results = []
        
        # Check for null values in critical fields
        null_transactions = transactions_df.filter(col("transaction_id").isNull()).count()
        null_events = events_df.filter(col("event_id").isNull()).count()
        
        quality_results.append({
            "check": "null_transaction_ids",
            "result": null_transactions,
            "status": "PASS" if null_transactions == 0 else "FAIL"
        })
        
        quality_results.append({
            "check": "null_event_ids", 
            "result": null_events,
            "status": "PASS" if null_events == 0 else "FAIL"
        })
        
        # Check for duplicate records
        total_transactions = transactions_df.count()
        unique_transactions = transactions_df.dropDuplicates(["transaction_id"]).count()
        
        quality_results.append({
            "check": "duplicate_transactions",
            "result": total_transactions - unique_transactions,
            "status": "PASS" if total_transactions == unique_transactions else "FAIL"
        })
        
        # Check data freshness (data should be from last 7 days)
        recent_data = transactions_df.filter(
            col("transaction_date") >= date_sub(current_date(), 7)
        ).count()
        
        quality_results.append({
            "check": "data_freshness",
            "result": recent_data,
            "status": "PASS" if recent_data > 0 else "FAIL"
        })
        
        # Print results
        print("Data Quality Check Results:")
        for result in quality_results:
            print(f"  {result['check']}: {result['status']} ({result['result']})")
        
        # Write quality results to S3
        quality_df = spark.createDataFrame(quality_results)
        quality_df.coalesce(1) \
                  .write \
                  .mode("overwrite") \
                  .json(f"s3a://{processed_bucket}/quality_reports/")
        
        return quality_results
    
    if __name__ == "__main__":
        spark = create_spark_session()
        try:
            results = run_data_quality_checks(spark)
            # Exit with error code if any checks failed
            failed_checks = [r for r in results if r['status'] == 'FAIL']
            if failed_checks:
                print(f"Data quality checks failed: {len(failed_checks)} failures")
                exit(1)
            else:
                print("All data quality checks passed!")
        finally:
            spark.stop()
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
  namespace: data-pipeline
data:
  AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
  AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "false"
  AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
  AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "30"
  AIRFLOW__CORE__PARALLELISM: "32"
  AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "16"
  AIRFLOW__WEBSERVER__WORKERS: "4"
  AIRFLOW__WEBSERVER__WORKER_TIMEOUT: "120"
  AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "8080"
  AIRFLOW__CORE__DAGS_FOLDER: "/opt/airflow/dags"
  AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/opt/airflow/logs"
  # Note: Database connection string with password should be set via environment variable in deployment
  # AIRFLOW__DATABASE__SQL_ALCHEMY_CONN will be constructed from secrets in the deployment
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-config
  namespace: data-storage
data:
  KAFKA_BROKER_ID: "1"
  KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT"
  KAFKA_LISTENERS: "PLAINTEXT://kafka:9092"
  KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
  KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
  KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "1"
  KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
  KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
  KAFKA_NUM_PARTITIONS: "3"
  KAFKA_DEFAULT_REPLICATION_FACTOR: "1"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: data-pipeline-config
  namespace: data-pipeline
data:
  POSTGRES_HOST: "postgres.data-storage.svc.cluster.local"
  POSTGRES_PORT: "5432"
  POSTGRES_DB: "transactions_db"
  CLICKHOUSE_HOST: "clickhouse.data-storage.svc.cluster.local"
  CLICKHOUSE_PORT: "8123"
  CLICKHOUSE_DB: "analytics"
  KAFKA_BOOTSTRAP_SERVERS: "kafka.data-storage.svc.cluster.local:9092"
  KAFKA_TOPIC_EVENTS: "user_events"
  KAFKA_TOPIC_TRANSACTIONS: "transactions"
  NUM_USERS: "10000"
  NUM_PRODUCTS: "1000"
  DAYS_OF_DATA: "90"
  BATCH_SIZE: "100"
  INTERVAL_SECONDS: "60"
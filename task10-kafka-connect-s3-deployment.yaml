apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-connect-s3-config
  namespace: data-ingestion
  labels:
    app: kafka-connect-s3
    component: configuration
data:
  # Kafka Connect Worker Configuration for S3 Sink
  connect-distributed.properties: |
    # Kafka Connection
    bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    
    # Worker Configuration
    group.id=connect-s3-cluster
    
    # Internal Topics
    config.storage.topic=connect-s3-configs
    config.storage.replication.factor=3
    offset.storage.topic=connect-s3-offsets
    offset.storage.replication.factor=3
    offset.storage.partitions=25
    status.storage.topic=connect-s3-status
    status.storage.replication.factor=3
    status.storage.partitions=5
    
    # Converters - Avro with Schema Registry
    key.converter=io.confluent.connect.avro.AvroConverter
    value.converter=io.confluent.connect.avro.AvroConverter
    key.converter.schema.registry.url=http://schema-registry.data-ingestion.svc.cluster.local:8081
    value.converter.schema.registry.url=http://schema-registry.data-ingestion.svc.cluster.local:8081
    key.converter.basic.auth.credentials.source=USER_INFO
    key.converter.basic.auth.user.info=admin:admin-secret
    value.converter.basic.auth.credentials.source=USER_INFO
    value.converter.basic.auth.user.info=admin:admin-secret
    
    # Internal Converters
    internal.key.converter=org.apache.kafka.connect.json.JsonConverter
    internal.value.converter=org.apache.kafka.connect.json.JsonConverter
    internal.key.converter.schemas.enable=false
    internal.value.converter.schemas.enable=false
    
    # REST API Configuration
    rest.port=8083
    rest.host.name=0.0.0.0
    rest.advertised.host.name=kafka-connect-s3
    rest.advertised.port=8083
    
    # Plugin Configuration
    plugin.path=/kafka/connect,/usr/share/java/kafka,/usr/share/java/confluent-common
    
    # Worker Settings
    task.shutdown.graceful.timeout.ms=10000
    offset.flush.interval.ms=10000
    offset.flush.timeout.ms=5000
    
    # Producer Settings
    producer.bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    producer.compression.type=lz4
    producer.max.request.size=1048576
    producer.buffer.memory=33554432
    producer.batch.size=16384
    producer.linger.ms=10
    producer.acks=1
    
    # Consumer Settings
    consumer.bootstrap.servers=kafka-headless.data-ingestion.svc.cluster.local:9092
    consumer.max.poll.records=10000
    consumer.max.poll.interval.ms=180000
    consumer.session.timeout.ms=30000
    consumer.heartbeat.interval.ms=3000
    consumer.auto.offset.reset=earliest
    
    # Error Handling
    errors.retry.timeout=300000
    errors.retry.delay.max.ms=60000
    errors.tolerance=none
    
    # Logging
    connect.logs.dir=/kafka/logs

  # JVM Configuration optimized for S3 workloads
  KAFKA_HEAP_OPTS: "-Xms512m -Xmx3g"
  JAVA_TOOL_OPTIONS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=30 -XX:+ExitOnOutOfMemoryError"
  CONNECT_BOOTSTRAP_SERVERS: "kafka-headless.data-ingestion.svc.cluster.local:9092"
  CONNECT_GROUP_ID: "connect-s3-cluster"
  CONNECT_CONFIG_STORAGE_TOPIC: "connect-s3-configs"
  CONNECT_OFFSET_STORAGE_TOPIC: "connect-s3-offsets"
  CONNECT_STATUS_STORAGE_TOPIC: "connect-s3-status"
  CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
  CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
  CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry.data-ingestion.svc.cluster.local:8081"
  CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry.data-ingestion.svc.cluster.local:8081"
  CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect-s3"
  CONNECT_REST_PORT: "8083"
  CONNECT_PLUGIN_PATH: "/kafka/connect,/usr/share/java/kafka,/usr/share/java/confluent-common"
  CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
  KAFKA_OPTS: "-Dlog4j.configuration=file:/kafka/config/connect-log4j.properties"
  CONFLUENT_TELEMETRY_ENABLED: "false"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: kafka-connect-s3-topics-setup
  namespace: data-ingestion
  labels:
    app: kafka-connect-s3
    component: topic-setup
spec:
  template:
    metadata:
      labels:
        app: kafka-connect-s3
        component: topic-setup
    spec:
      serviceAccountName: kafka-sa
      restartPolicy: OnFailure
      containers:
      - name: kafka-topics-setup
        image: confluentinc/cp-kafka:7.4.0
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "500m"
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Setting up Kafka Connect S3 internal topics..."
          
          # Wait for Kafka to be ready
          TIMEOUT=300
          ELAPSED=0
          until kafka-topics --bootstrap-server kafka-0.kafka-headless.data-ingestion.svc.cluster.local:9092 --list > /dev/null 2>&1 || [ $ELAPSED -ge $TIMEOUT ]; do
            echo "Waiting for Kafka to be ready... ($ELAPSED seconds elapsed)"
            sleep 5
            ELAPSED=$((ELAPSED + 5))
          done
          
          if [ $ELAPSED -ge $TIMEOUT ]; then
            echo "ERROR: Kafka failed to become ready within $TIMEOUT seconds"
            exit 1
          fi
          
          # Create Kafka Connect S3 internal topics
          echo "Creating connect-s3-configs topic..."
          kafka-topics --bootstrap-server kafka-0.kafka-headless.data-ingestion.svc.cluster.local:9092 \
            --create --if-not-exists \
            --topic connect-s3-configs \
            --partitions 1 \
            --replication-factor 3 \
            --config cleanup.policy=compact \
            --config compression.type=lz4 \
            --config min.insync.replicas=2
          
          echo "Creating connect-s3-offsets topic..."
          kafka-topics --bootstrap-server kafka-0.kafka-headless.data-ingestion.svc.cluster.local:9092 \
            --create --if-not-exists \
            --topic connect-s3-offsets \
            --partitions 25 \
            --replication-factor 3 \
            --config cleanup.policy=compact \
            --config compression.type=lz4 \
            --config min.insync.replicas=2
          
          echo "Creating connect-s3-status topic..."
          kafka-topics --bootstrap-server kafka-0.kafka-headless.data-ingestion.svc.cluster.local:9092 \
            --create --if-not-exists \
            --topic connect-s3-status \
            --partitions 5 \
            --replication-factor 3 \
            --config cleanup.policy=compact \
            --config compression.type=lz4 \
            --config min.insync.replicas=2
          
          # Create S3 Sink Dead Letter Queue topic
          echo "Creating s3-sink-dlq topic..."
          kafka-topics --bootstrap-server kafka-0.kafka-headless.data-ingestion.svc.cluster.local:9092 \
            --create --if-not-exists \
            --topic s3-sink-dlq \
            --partitions 3 \
            --replication-factor 3 \
            --config retention.ms=604800000 \
            --config compression.type=lz4 \
            --config cleanup.policy=delete \
            --config min.insync.replicas=2
          
          echo "Kafka Connect S3 topics created successfully!"
          echo "Current topics:"
          kafka-topics --bootstrap-server kafka-0.kafka-headless.data-ingestion.svc.cluster.local:9092 --list
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-connect-s3
  namespace: data-ingestion
  labels:
    app: kafka-connect-s3
    component: worker
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-connect-s3
      component: worker
  template:
    metadata:
      labels:
        app: kafka-connect-s3
        component: worker
    spec:
      serviceAccountName: kafka-connect-sa
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      initContainers:
      # Install S3 Sink connector plugin
      - name: plugin-installer
        image: confluentinc/cp-kafka-connect:7.4.0
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "Installing Confluent S3 Sink connector plugin..."
          
          # Create plugin directory
          mkdir -p /kafka/connect/kafka-connect-s3
          
          # Download and install S3 Sink connector
          cd /tmp
          curl -L -o kafka-connect-s3.zip \
            "https://api.hub.confluent.io/api/plugins/confluentinc/kafka-connect-s3/versions/10.5.0/archive"
          
          unzip kafka-connect-s3.zip
          cp -r confluentinc-kafka-connect-s3-*/* /kafka/connect/kafka-connect-s3/
          
          echo "S3 Sink connector installed successfully!"
          ls -la /kafka/connect/kafka-connect-s3/
        volumeMounts:
        - name: plugin-volume
          mountPath: /kafka/connect
      containers:
      - name: kafka-connect-s3
        image: confluentinc/cp-kafka-connect:7.4.0
        command: ["/bin/bash"]
        args: ["/etc/confluent/docker/run"]
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        envFrom:
        - configMapRef:
            name: kafka-connect-s3-config
        ports:
        - name: http
          containerPort: 8083
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "768Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /
            port: 8083
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /connectors
            port: 8083
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /
            port: 8083
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
        volumeMounts:
        - name: config-volume
          mountPath: /kafka/config
        - name: plugin-volume
          mountPath: /kafka/connect
        - name: logs-volume
          mountPath: /kafka/logs
        - name: aws-credentials
          mountPath: /var/run/secrets/aws-credentials
          readOnly: true
      volumes:
      - name: config-volume
        configMap:
          name: kafka-connect-s3-config
      - name: plugin-volume
        emptyDir: {}
      - name: logs-volume
        emptyDir: {}
      - name: aws-credentials
        secret:
          secretName: aws-credentials
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-connect-s3
  namespace: data-ingestion
  labels:
    app: kafka-connect-s3
    component: service
spec:
  type: ClusterIP
  selector:
    app: kafka-connect-s3
    component: worker
  ports:
  - name: http
    protocol: TCP
    port: 8083
    targetPort: 8083
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-connect-s3-nodeport
  namespace: data-ingestion
  labels:
    app: kafka-connect-s3
    component: external-service
spec:
  type: NodePort
  selector:
    app: kafka-connect-s3
    component: worker
  ports:
  - name: http
    port: 8083
    targetPort: 8083
    nodePort: 30084
    protocol: TCP